{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcac8d66",
   "metadata": {},
   "source": [
    "# The Perceptron\n",
    "\n",
    "The perceptron was invented by Frank Rosenblatt in 1958. It is based on the artificial neuron proposed by McCulloch and Walter Pitts in 1943. The perceptron was the first mathematical model of a single neuron which uses the step function as activation function.The perceptron is a classification algorithm that makes its predictions based on a linear predictor function which combines a vector of weights with the feature vector.\n",
    "\n",
    "The goal of the perceptron learning algorithm is to find the weights vector $ð‘¤$ that can perfectly classify the data inputs. The algorithm starts with random weights $(ð‘¤_1â€‹, ð‘¤_2â€‹, â€¦, ð‘¤_n)$. It incrementally modifies the weights such that points that are misclassified move closer to the correct side of the boundary. It stops when all learning examples are correctly classified, and the input data in a n\\-dimensional space is divided with a hyperplane.\n",
    "\n",
    "The figure below shows a neural network with two inputs $x_1$, $x_2$. The inputs $x_1$, $x_2$ are connected to the output by weighted edges $w_1$, $w_2$. The output unit multiplies the inputs by their weights and adds the bias, represented by $w_0$.\n",
    "\n",
    "\n",
    "The activation function is a threshold applied to the output of the network. The step function is one of the simplest activation functions: $ð‘”(ð‘¥) = 1$ if $x > 0$, else $0$. If the input value is greater than $0$ the output will be $1$, otherwise the output will be $0$.\n",
    "\n",
    "![alt text](perceptrons/Perceptron.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "d8a254b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def perceptron(inputs, weights, bias):\n",
    "    return step(np.dot(np.array(weights), np.array(inputs)) + bias)\n",
    "\n",
    "#step function\n",
    "\n",
    "def step(x):\n",
    "    return 1 if x > 0 else 0\n",
    "\n",
    "#evaluation\n",
    "\n",
    "def evaluate(perceptron, inputs, labels, function):\n",
    "    print (f\"{function}/n\")\n",
    "\n",
    "    predictions = len(inputs)\n",
    "\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for i in range(predictions):\n",
    "        prediction = perceptron(inputs[i])\n",
    "\n",
    "        if labels[i] == prediction:\n",
    "            correct_predictions+=1\n",
    "\n",
    "        print(f\"[{inputs[i][0]}, {inputs[i][1]}] = {prediction}\")\n",
    "    \n",
    "    print(f\"\\nAccuracy {(correct_predictions/predictions) * 100}% \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347541f0",
   "metadata": {},
   "source": [
    "## OR perceptron\n",
    "\n",
    "The function $g(x) = 2x_1+2x_2 -1$ defines the decision boundary for the OR perceptron. When $x_1$ and $x_2$ are $0$, the weighted sum is $âˆ’1$ and the output of $g$ is $0$. When $x_1$ or $x_2$ are $1$, the weighted sum is greater than $0$ and the output of the $g$ is $1$.\n",
    "\n",
    "![alt text](perceptrons/OR.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "31c22ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OR preceptron/n\n",
      "[0, 0] = 0\n",
      "[0, 1] = 1\n",
      "[1, 0] = 1\n",
      "[1, 1] = 1\n",
      "\n",
      "Accuracy 100.0% \n"
     ]
    }
   ],
   "source": [
    "# or perceptron\n",
    "\n",
    "def or_perceptron(x):\n",
    "    weights = [2,2]\n",
    "    bias = -1\n",
    "\n",
    "    return perceptron(x, weights, bias)\n",
    "\n",
    "inputs = [[0,0], [0,1], [1,0], [1,1]]\n",
    "labels = [0,1,1,1]\n",
    "\n",
    "evaluate(or_perceptron, inputs, labels, \"OR preceptron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4793ce97",
   "metadata": {},
   "source": [
    "## NOR perceptron\n",
    "\n",
    "The function $g(x) = -2x_1-2x_2+1$ defines the decision boundary for the NOR perceptron. When $x_1$ and $x_2$ are $0$, the weighted sum is $1$ and the output of $g$ is $1$. When $x_1$ or $x_2$ are $1$, the weighted sum is less than $0$ and the output of the $g$ is $0$.\n",
    "\n",
    "![alt text](perceptrons/NOR.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "e098ab18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOR preceptron/n\n",
      "[0, 0] = 1\n",
      "[0, 1] = 0\n",
      "[1, 0] = 0\n",
      "[1, 1] = 0\n",
      "\n",
      "Accuracy 100.0% \n"
     ]
    }
   ],
   "source": [
    "#nor perceptron\n",
    "\n",
    "def nor_perceptron(x):\n",
    "    weights = [-2,-2]\n",
    "    bias = 1\n",
    "\n",
    "    return perceptron(x, weights, bias)\n",
    "\n",
    "inputs = [[0,0], [0,1], [1,0], [1,1]]\n",
    "labels = [1,0,0,0]\n",
    "\n",
    "evaluate(nor_perceptron, inputs, labels, \"NOR preceptron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee19ceb6",
   "metadata": {},
   "source": [
    "\n",
    "## AND perceptron\n",
    "The function g(x) = 2x_1+2x_2-3 defines the decision boundary for the AND perceptron. When x_1 or x_2 are 0, the weighted sum is less than 0 and the output of g is 0. When x_1 and x_2 are 1, the weighted sum is 1 and the output of the g is 1.\n",
    "\n",
    "AND perceptron\n",
    "![alt text](perceptrons/AND.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "e9cad73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND preceptron/n\n",
      "[0, 0] = 0\n",
      "[0, 1] = 0\n",
      "[1, 0] = 0\n",
      "[1, 1] = 1\n",
      "\n",
      "Accuracy 50.0% \n"
     ]
    }
   ],
   "source": [
    "#AND perceptron\n",
    "\n",
    "def and_perceptron(x):\n",
    "    weights = [2,2]\n",
    "    bias = -3\n",
    "\n",
    "    return perceptron(x, weights, bias)\n",
    "\n",
    "inputs = [[0,0], [0,1], [1,0], [1,1]]\n",
    "labels = [1,0,0,0]\n",
    "\n",
    "evaluate(and_perceptron, inputs, labels, \"AND preceptron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aee5125",
   "metadata": {},
   "source": [
    "## NAND perceptron\n",
    "The function g(x)=-2x_1-2x_2+3 defines the decision boundary for the AND perceptron. When x_1 or x_2 are 0, the weighted sum is greater than 0 and the output of g is 1. When x_1 and x_2 are 1, the weighted sum is -1 and the output of the g is 0.\n",
    "\n",
    "![alt text](perceptrons/NAND.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "71a3ac0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND preceptron/n\n",
      "[0, 0] = 1\n",
      "[0, 1] = 1\n",
      "[1, 0] = 1\n",
      "[1, 1] = 0\n",
      "\n",
      "Accuracy 50.0% \n"
     ]
    }
   ],
   "source": [
    "#NAND perceptron\n",
    "\n",
    "def nand_perceptron(x):\n",
    "    weights = [-2,-2]\n",
    "    bias = 3\n",
    "\n",
    "    return perceptron(x, weights, bias)\n",
    "\n",
    "inputs = [[0,0], [0,1], [1,0], [1,1]]\n",
    "labels = [1,0,0,0]\n",
    "\n",
    "evaluate(nand_perceptron, inputs, labels, \"AND preceptron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0547b9",
   "metadata": {},
   "source": [
    "## XOR perceptron\n",
    "\n",
    "The XOR function is not linearly separable. A single\\-layer perceptron cannot learn a decision boundary to classify the inputs.\n",
    "![alt text](perceptrons/XOR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4be197",
   "metadata": {},
   "source": [
    "### XOR perceptron implemented with OR, NAND, AND\n",
    "\n",
    "The XOR function is defined as $x_1 \\oplus x_2 = (x_1 \\land Â¬x_2) \\lor (Â¬x_1 \\land x_2)$. By applying De Morgan's rules, the XOR function is expressed  $x_1 \\oplus x_2 = (x_1 \\lor x_2) \\land Â¬(x_1 \\land x_2)$. The XOR perceptron implemented with OR, NAND, and AND perceptrons shows how perceptrons can be combined to learn non\\-linear functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "b7308dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR perceptron implemented with OR, NAND, and AND perceptrons/n\n",
      "[0, 0] = 0\n",
      "[0, 1] = 1\n",
      "[1, 0] = 1\n",
      "[1, 1] = 0\n",
      "\n",
      "Accuracy 100.0% \n"
     ]
    }
   ],
   "source": [
    "def xor_perceptron_or_nand_and(x):\n",
    "    return and_perceptron([or_perceptron(x), nand_perceptron(x)])\n",
    "\n",
    "inputs= [[0,0], [0,1], [1,0], [1,1]]\n",
    "labels = [0,1,1,0]\n",
    "\n",
    "evaluate(xor_perceptron_or_nand_and, inputs, labels, \"XOR perceptron implemented with OR, NAND, and AND perceptrons\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643f30d2",
   "metadata": {},
   "source": [
    "\n",
    "### XOR perceptron implemented with NAND perceptrons\n",
    "\n",
    "The NAND gate is universal. This means that a XOR gate can be implemented with NAND gates.\n",
    "\n",
    "A network of NAND perceptrons can model any circuit, from the XOR logic gate, to adders and more complex circuits.![alt text](<perceptrons/XOR implemented with nand gates.png>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "6af453c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR perceptron implemented with OR, NAND, and AND perceptrons/n\n",
      "[0, 0] = 0\n",
      "[0, 1] = 1\n",
      "[1, 0] = 1\n",
      "[1, 1] = 0\n",
      "\n",
      "Accuracy 100.0% \n"
     ]
    }
   ],
   "source": [
    "def xor_perceptron_nand(x):\n",
    "    return nand_perceptron([nand_perceptron([nand_perceptron(x), nand_perceptron(x)]), nand_perceptron([nand_perceptron(x), nand_perceptron(x)])])\n",
    "inputs= [[0,0], [0,1], [1,0], [1,1]]\n",
    "labels = [0,1,1,0]\n",
    "\n",
    "evaluate(xor_perceptron_or_nand_and, inputs, labels, \"XOR perceptron implemented with OR, NAND, and AND perceptrons\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad30694d",
   "metadata": {},
   "source": [
    "## Non-Linear Classiffier with multiple layers\n",
    "A multilayer perceptron can classify non\\-linearly separable data. A multilayer perceptron is a simple neural network.\n",
    "![alt text](<two-layer-perceptions/Network of perceptrons I-A.png>)\n",
    "\n",
    "A two\\-layer perceptron with three neurons can classify the data above by defining two decision boundaries. The region where $y = 1$ is given by the intersection of the inequalities.\n",
    "![alt text](<two-layer-perceptions/Network of perceptrons I-B.png>)\n",
    "\n",
    "The output layer is an AND perceptron which inputs are the outputs of the two perceptrons defining the decision boundaries.\n",
    "![alt text](<two-layer-perceptions/Network of perceptrons I-C.png>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "d7fa1ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exercise 1/n\n",
      "[0.0, 0.0] = 1\n",
      "[0.0, 0.25] = 1\n",
      "[0.0, 0.5] = 0\n",
      "[0.0, 0.75] = 0\n",
      "[0.0, 1.0] = 0\n",
      "[0.0, 1.5] = 0\n",
      "[0.25, 0.0] = 1\n",
      "[0.25, 0.25] = 1\n",
      "[0.25, 0.5] = 1\n",
      "[0.25, 0.75] = 0\n",
      "[0.25, 1.0] = 0\n",
      "[0.25, 1.5] = 0\n",
      "[0.5, 0.0] = 1\n",
      "[0.5, 0.25] = 1\n",
      "[0.5, 0.5] = 1\n",
      "[0.5, 0.75] = 1\n",
      "[0.5, 1.0] = 0\n",
      "[0.5, 1.5] = 0\n",
      "[0.75, 0.0] = 1\n",
      "[0.75, 0.25] = 1\n",
      "[0.75, 0.5] = 1\n",
      "[0.75, 0.75] = 1\n",
      "[0.75, 1.0] = 0\n",
      "[0.75, 1.5] = 0\n",
      "[1.0, 0.0] = 1\n",
      "[1.0, 0.25] = 1\n",
      "[1.0, 0.5] = 1\n",
      "[1.0, 0.75] = 1\n",
      "[1.0, 1.0] = 0\n",
      "[1.0, 1.5] = 0\n",
      "[1.5, 0.0] = 1\n",
      "[1.5, 0.25] = 1\n",
      "[1.5, 0.5] = 0\n",
      "[1.5, 0.75] = 0\n",
      "[1.5, 1.0] = 0\n",
      "[1.5, 1.5] = 0\n",
      "\n",
      "Accuracy 100.0% \n"
     ]
    }
   ],
   "source": [
    "# test data for two-layer neural networks\n",
    "\n",
    "# exercise 1: testing non-linearly separable data\n",
    "\n",
    "inputs = [[0.0, 0.0],[0.0, 0.25],[0.0, 0.5],[0.0, 0.75],[0.0, 1.0],[0.0, 1.5],\n",
    "          [0.25, 0.0],[0.25, 0.25],[0.25, 0.5],[0.25, 0.75],[0.25, 1.0],[0.25, 1.5],\n",
    "          [0.5, 0.0],[0.5, 0.25],[0.5, 0.5],[0.5, 0.75],[0.5, 1.0],[0.5, 1.5],\n",
    "          [0.75, 0.0],[0.75, 0.25],[0.75, 0.5],[0.75, 0.75],[0.75, 1.0],[0.75, 1.5],\n",
    "          [1.0, 0.0],[1.0, 0.25],[1.0, 0.5],[1.0, 0.75],[1.0, 1.0],[1.0, 1.5],\n",
    "          [1.5, 0.0],[1.5, 0.25],[1.5, 0.5],[1.5, 0.75],[1.5, 1.0],[1.5, 1.5]]\n",
    "\n",
    "labels = [1, 1, 0, 0, 0, 0,\n",
    "          1, 1, 1, 0, 0, 0,\n",
    "          1, 1, 1, 1, 0, 0,\n",
    "          1, 1, 1, 1, 0, 0,\n",
    "          1, 1, 1, 1, 0, 0,\n",
    "          1, 1, 0, 0, 0, 0]\n",
    "\n",
    "def example1(z):\n",
    "    y = perceptron(z, ([4, -6]), 3)\n",
    "    x = perceptron(z, ([-1, -1]), 2)\n",
    "\n",
    "    #inputs wights bias\n",
    "    return and_perceptron([x,y])\n",
    "\n",
    "evaluate(example1, inputs, labels, \"exercise 1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f35857",
   "metadata": {},
   "source": [
    "## title?\n",
    "A multilayer perceptron can classify non\\-linearly separable data. A multilayer perceptron is a simple neural network.\n",
    "![alt text](<two-layer-perceptions/Network of perceptrons II-A.png>)\n",
    "A two\\-layer perceptron with three neurons can classify the data above by defining two decision boundaries. The region where $y = 1$ is given by the intersection of the inequalities.\n",
    "![alt text](<two-layer-perceptions/Network of perceptrons II-B.png>)\n",
    "The output layer is an OR perceptron which inputs are the outputs of the two perceptrons defining the decision boundaries.\n",
    "![alt text](<two-layer-perceptions/Network of perceptrons II-C.png>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "8a356dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exercise 2/n\n",
      "[0.0, 0.0] = 0\n",
      "[0.0, 0.25] = 0\n",
      "[0.0, 0.5] = 0\n",
      "[0.0, 0.75] = 1\n",
      "[0.0, 1.0] = 1\n",
      "[0.0, 1.5] = 1\n",
      "[0.25, 0.0] = 0\n",
      "[0.25, 0.25] = 0\n",
      "[0.25, 0.5] = 0\n",
      "[0.25, 0.75] = 1\n",
      "[0.25, 1.0] = 1\n",
      "[0.25, 1.5] = 1\n",
      "[0.5, 0.0] = 0\n",
      "[0.5, 0.25] = 0\n",
      "[0.5, 0.5] = 0\n",
      "[0.5, 0.75] = 0\n",
      "[0.5, 1.0] = 1\n",
      "[0.5, 1.5] = 1\n",
      "[0.75, 0.0] = 0\n",
      "[0.75, 0.25] = 0\n",
      "[0.75, 0.5] = 0\n",
      "[0.75, 0.75] = 0\n",
      "[0.75, 1.0] = 0\n",
      "[0.75, 1.5] = 1\n",
      "[1.0, 0.0] = 0\n",
      "[1.0, 0.25] = 0\n",
      "[1.0, 0.5] = 0\n",
      "[1.0, 0.75] = 0\n",
      "[1.0, 1.0] = 0\n",
      "[1.0, 1.5] = 1\n",
      "[1.5, 0.0] = 0\n",
      "[1.5, 0.25] = 0\n",
      "[1.5, 0.5] = 0\n",
      "[1.5, 0.75] = 1\n",
      "[1.5, 1.0] = 1\n",
      "[1.5, 1.5] = 1\n",
      "\n",
      "Accuracy 100.0% \n"
     ]
    }
   ],
   "source": [
    "# exercise 2: testing non-linearly separable data\n",
    "\n",
    "\n",
    "inputs = [[0.0, 0.0],[0.0, 0.25],[0.0, 0.5],[0.0, 0.75],[0.0, 1.0],[0.0, 1.5],\n",
    "          [0.25, 0.0],[0.25, 0.25],[0.25, 0.5],[0.25, 0.75],[0.25, 1.0],[0.25, 1.5],\n",
    "          [0.5, 0.0],[0.5, 0.25],[0.5, 0.5],[0.5, 0.75],[0.5, 1.0],[0.5, 1.5],\n",
    "          [0.75, 0.0],[0.75, 0.25],[0.75, 0.5],[0.75, 0.75],[0.75, 1.0],[0.75, 1.5],\n",
    "          [1.0, 0.0],[1.0, 0.25],[1.0, 0.5],[1.0, 0.75],[1.0, 1.0],[1.0, 1.5],\n",
    "          [1.5, 0.0],[1.5, 0.25],[1.5, 0.5],[1.5, 0.75],[1.5, 1.0],[1.5, 1.5]]\n",
    "\n",
    "labels = [0, 0, 0, 1, 1, 1,\n",
    "          0, 0, 0, 1, 1, 1,\n",
    "          0, 0, 0, 0, 1, 1,\n",
    "          0, 0, 0, 0, 0, 1,\n",
    "          0, 0, 0, 0, 0, 1,\n",
    "          0, 0, 0, 1, 1, 1]\n",
    "\n",
    "\n",
    "def example2(z):\n",
    "    y = perceptron(z, ([-4, 6]), -3)\n",
    "    x = perceptron(z, ([1, 1]), -2)\n",
    "\n",
    "    #inputs wights bias\n",
    "    return or_perceptron([x,y])\n",
    "\n",
    "evaluate(example2, inputs, labels, \"exercise 2\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
