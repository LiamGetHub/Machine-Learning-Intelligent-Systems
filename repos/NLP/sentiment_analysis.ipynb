{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "Sentiment analysis is a popular use case of text classification. Sentiment analysis focuses on categorizing opinions expressed in a piece of text, to determine whether it is positive, negative, or neutral. It has a lot of business applications, such as product analytics, brand monitoring, market research, or customer support. Sentiment analysis uses natural language processing \\(NLP\\) and machine learning models such as Naive Bayes.\n",
    "\n",
    "## Naive Bayes\n",
    "\n",
    "Naive Bayes is a classification algorithm based on the Bayes Theorem. It is naive because it assumes the conditional independence of words. This is, the presence of a particular word in a class \\(category\\) is not related to the presence of any other word to simplify computations. Naive Bayes is a technique that can be used in sentiment analysis with the bag of words model. This algorithm is used to calculate the probability that a piece of text is positive or negative given the words in the sentence. Naive Bayes is simple, but it can outperform more sophisticated classification methods.\n",
    "\n",
    "Bayesâ€™ rule is used in probability theory to calculate conditional probability. Bayesâ€™ rule says that the probability of $b$ given $a$ is equal to the probability of $a$ given $b$, times the probability of $b$, divided by the probability of $a$.\n",
    "\n",
    "<br/>\n",
    "\n",
    "$P(b \\; | \\; a) = \\large{\\frac{P(b) \\; P(a \\; | \\; b)}{P(a)}}$\n",
    "\n",
    "<br/>\n",
    "\n",
    "Bayesâ€™ rule is a powerful rule of probability and statistics. Knowing $ğ‘ƒ(ğ‘ \\; | \\; ğ‘)$, $ğ‘ƒ(ğ‘)$ and $ğ‘ƒ(ğ‘)$, it calculates $ğ‘ƒ(ğ‘ \\; | \\; ğ‘)$.\n",
    "\n",
    "## Bag of words model\n",
    "\n",
    "The bag of words model represents text as an unordered collection of words, ignoring the syntax. This approach is used in text classification tasks where the frequency of each word is used as a feature by the classifier, such as sentiment analysis or email classification.\n",
    "\n",
    "## The Naive Bayes classifier\n",
    "\n",
    "Naive Bayes assumes that every word in a sentence is independent of the other words. This is, it assumes the context does not matter. Naive Bayes calculates $ğ‘ƒ(ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ \\; | \\;``ğ‘ğ‘šğ‘ğ‘§ğ‘–ğ‘›ğ‘” \\; ğ‘šğ‘œğ‘£ğ‘–ğ‘’\")$ as $ğ‘ƒ(ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ \\; | \\; ``ğ‘ğ‘šğ‘ğ‘§ğ‘–ğ‘›ğ‘”\", \\; ``ğ‘šğ‘œğ‘£ğ‘–ğ‘’\")$. This is equal to\n",
    "\n",
    "<br/>\n",
    "\n",
    "$\\large{\\frac{ğ‘ƒ(ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’) \\; ğ‘ƒ(``ğ‘ğ‘šğ‘ğ‘§ğ‘–ğ‘›ğ‘”\", \\; ``ğ‘šğ‘œğ‘£ğ‘–ğ‘’\" \\; | \\; ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’)}{ğ‘ƒ(``ğ‘ğ‘šğ‘ğ‘§ğ‘–ğ‘›ğ‘”\", \\; \"ğ‘šğ‘œğ‘£ğ‘–ğ‘’\")}}$\n",
    "\n",
    "<br/>\n",
    "\n",
    "The probability $ğ‘ƒ(ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’) \\; ğ‘ƒ(``ğ‘ğ‘šğ‘ğ‘§ğ‘–ğ‘›ğ‘”\", \\; ``ğ‘šğ‘œğ‘£ğ‘–ğ‘’\" \\; | \\; ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’)$ is proportional to $P(positive, \\; ``amazing\", \\; ``movie\")$ and naively proportional to $P(positive) \\; P(``amazing\" \\; | \\; positive) \\; P(``movie\" \\; | \\; positive)$, since Naive Bayes assumes that the context does not matter. The denominator of the equation is ignored to reduce computations. The probability will be normalized to sump up to $1$.\n",
    "\n",
    "Therefore, Naive Bayes calculates the probability that a document containing words $w_1$, $ğ‘¤_2$, â€¦, $ğ‘¤_ğ‘›$ has a positive sentiment and negative sentiment as:\n",
    "\n",
    "$pos = P(positive \\; | w_1, w_2, ... , w_n) = P(positive) \\prod_{i=1}^{n} P(w_i \\; | \\; positive)$\n",
    "\n",
    "$neg = P(negative \\; | w_1, w_2, ... , w_n) = P(negative) \\prod_{i=1}^{n} P(w_i \\; | \\; negative)$\n",
    "\n",
    "<br/>\n",
    "\n",
    "Where $P(positive) = \\large{\\frac{positive \\; reviews}{total reviews}}$, and $P(negative) = \\large{\\frac{negative \\; reviews}{total reviews}}$\n",
    "\n",
    "<br/>\n",
    "\n",
    "Probability is calculated using additive smoothing to avoid words with probability zero. Laplace smoothing adds $1$ to each value in the probability distribution, pretending that all words ğ‘¤ have been observed at least once.\n",
    "\n",
    "$ğ‘ƒ(ğ‘¤_ğ‘– \\; | \\; ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’) = \\large{\\frac{ğ‘“ğ‘Ÿğ‘’ğ‘ğ‘¢ğ‘’ğ‘›ğ‘ğ‘¦ \\; ğ‘œğ‘“ \\; ğ‘¤_ğ‘– \\; ğ‘–ğ‘› \\; ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ \\; reviews \\; + \\; 1}{ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ \\; ğ‘¤ğ‘œğ‘Ÿğ‘‘ğ‘  \\; ğ‘–ğ‘› \\; ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ \\; reviews \\; + \\; ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ \\; ğ‘¢ğ‘›ğ‘–ğ‘ğ‘¢ğ‘’ \\; ğ‘¤ğ‘œğ‘Ÿğ‘‘ğ‘ }}$\n",
    "\n",
    "$ğ‘ƒ(ğ‘¤_ğ‘– \\; | \\; negağ‘¡ğ‘–ğ‘£ğ‘’) = \\large{\\frac{ğ‘“ğ‘Ÿğ‘’ğ‘ğ‘¢ğ‘’ğ‘›ğ‘ğ‘¦ \\; ğ‘œğ‘“ \\; ğ‘¤_ğ‘– \\; ğ‘–ğ‘› \\; negağ‘¡ğ‘–ğ‘£ğ‘’ \\; reviews \\; + \\; 1}{ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ \\; ğ‘¤ğ‘œğ‘Ÿğ‘‘ğ‘  \\; ğ‘–ğ‘› \\; negağ‘¡ğ‘–ğ‘£ğ‘’ \\; reviews \\; + \\; ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ \\; ğ‘¢ğ‘›ğ‘–ğ‘ğ‘¢ğ‘’ \\; ğ‘¤ğ‘œğ‘Ÿğ‘‘ğ‘ }}$\n",
    "\n",
    "<br/>\n",
    "\n",
    "The normalized probability is:\n",
    "\n",
    "$positive \\; sentiment = \\frac{pos}{pos \\; + \\; neg}$\n",
    "\n",
    "$negative \\; sentiment = \\frac{neg}{pos \\; + \\; neg}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_words returns a list with the words in a text\n",
    "\n",
    "def extract_words(text):\n",
    "    return list(\n",
    "        word.lower() for word in word_tokenize(text)\n",
    "        if any(c.isalpha() for c in word)\n",
    "    )\n",
    "\n",
    "# load_file returns a tuple with the list of words in the input file and the total lines read, the function extend() adds multiple items to a list\n",
    "\n",
    "def load_file(file_name):\n",
    "    total_lines = 0\n",
    "\n",
    "    words = []\n",
    "\n",
    "    with open(file_name, \"r\") as f:\n",
    "        for line in f:\n",
    "            words.extend(extract_words(line))\n",
    "\n",
    "            total_lines += 1\n",
    "\n",
    "    return (words, total_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['great', 'best', 'movie', 'ever', 'wonderful', 'indie', 'production', 'great', 'movie', 'i', 'recommend', 'it', 'i', 'loved', 'it', 'my', 'children', 'loved', 'it', 'i', 'really', 'enjoyed', 'it', 'amazing', 'movie', 'i', 'had', 'a', 'good', 'time', 'i', 'enjoyed', 'it', 'so', 'much', 'beautiful', 'and', 'tender', 'story'] 10\n"
     ]
    }
   ],
   "source": [
    "#words in positive reviews\n",
    "\n",
    "words_in_positive_reviews, positive_reviews = load_file(\"positive_reviews.txt\")\n",
    "\n",
    "print(words_in_positive_reviews, positive_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['really', 'boring', 'worst', 'movie', 'ever', 'a', 'waste', 'of', 'time', 'not', 'worth', 'it', 'pretentious', 'and', 'boring', 'really', 'bad', 'i', 'want', 'my', 'money', 'back', 'terrible', 'a', 'waste', 'of', 'time', 'mediocre', 'production', 'and', 'direction', 'boring', 'and', 'mediocre', 'movie', 'very', 'bad', 'not', 'worth', 'it', 'tedious', 'story', 'and', 'characters', 'very', 'bad', 'and', 'boring'] 10\n"
     ]
    }
   ],
   "source": [
    "#words in negative reviews\n",
    "\n",
    "words_in_negative_reviews, negative_reviews = load_file(\"negative_reviews.txt\")\n",
    "\n",
    "print(words_in_negative_reviews, negative_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word_frequency(words_in_positive_reviews, words_in_negative_reviews):\n",
    "    # each entry corresponds with a word, and its value with the word's frequency in positive and negative reviews\n",
    "    # for example, 'enjoyed': [2, 0] indicates that 'enjoyed' appears 2 times in positive reviews and zero times in negative reviews\n",
    "\n",
    "    frequency = {}\n",
    "\n",
    "    # for each word in a positive review, if the word is not in the dictionary it adds a new entry with frequency [1, 0]\n",
    "    # otherwise, it increments the first value of the list, which corresponds to positive reviews\n",
    "\n",
    "    for word in words_in_positive_reviews:\n",
    "        if word in frequency.keys():\n",
    "            frequency[word][0] += 1\n",
    "        else:\n",
    "            frequency[word] = [1, 0]\n",
    "\n",
    "    # for each word in a negative review, if the word is not in the dictionary it adds a new entry with frequency [0, 1]\n",
    "    # otherwise, it increments the second value of the list, which corresponds to negative reviews\n",
    "\n",
    "    for word in words_in_negative_reviews:\n",
    "        if word in frequency.keys():\n",
    "            frequency[word][1] += 1\n",
    "        else:\n",
    "            frequency[word] = [0, 1]\n",
    "\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_word_probability returns a dictionary with word's probability in positive and negative reviews\n",
    "\n",
    "def calculate_word_probability(frequency, words_in_positive_reviews, words_in_negative_reviews):\n",
    "    # each entry corresponds with a word, and its value with the word's probability in positive and negative reviews\n",
    "    # for example, 'enjoyed': [0.05263157894736842, 0.015625] indicates that 'enjoyed' appears in positive reviews with\n",
    "    # probability 0.05263157894736842, and in negative reviews with probability 0.015625\n",
    "\n",
    "    # for each word in the dictionary frequency, it adds a new word in the dictionary probability, the probability of a\n",
    "    # word is calculated using additive smoothing to avoid probability zero. Laplace Smoothing adds 1 to each value in\n",
    "    # the probability distribution, pretending that all words have been observed at least once\n",
    "\n",
    "    probability = {}\n",
    "\n",
    "    for word in frequency:\n",
    "        positive = (frequency[word][0] + 1)/(len(words_in_positive_reviews) + len(frequency))\n",
    "        negative = (frequency[word][1] + 1)/(len(words_in_negative_reviews) + len(frequency))\n",
    "\n",
    "        probability[word] = [positive, negative]\n",
    "\n",
    "    return probability\t    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'great': [2, 0], 'best': [1, 0], 'movie': [3, 2], 'ever': [1, 1], 'wonderful': [1, 0], 'indie': [1, 0], 'production': [1, 1], 'i': [5, 1], 'recommend': [1, 0], 'it': [5, 2], 'loved': [2, 0], 'my': [1, 1], 'children': [1, 0], 'really': [1, 2], 'enjoyed': [2, 0], 'amazing': [1, 0], 'had': [1, 0], 'a': [1, 2], 'good': [1, 0], 'time': [1, 2], 'so': [1, 0], 'much': [1, 0], 'beautiful': [1, 0], 'and': [1, 5], 'tender': [1, 0], 'story': [1, 1], 'boring': [0, 4], 'worst': [0, 1], 'waste': [0, 2], 'of': [0, 2], 'not': [0, 2], 'worth': [0, 2], 'pretentious': [0, 1], 'bad': [0, 3], 'want': [0, 1], 'money': [0, 1], 'back': [0, 1], 'terrible': [0, 1], 'mediocre': [0, 2], 'direction': [0, 1], 'very': [0, 2], 'tedious': [0, 1], 'characters': [0, 1]}\n",
      "{'great': [0.036585365853658534, 0.01098901098901099], 'best': [0.024390243902439025, 0.01098901098901099], 'movie': [0.04878048780487805, 0.03296703296703297], 'ever': [0.024390243902439025, 0.02197802197802198], 'wonderful': [0.024390243902439025, 0.01098901098901099], 'indie': [0.024390243902439025, 0.01098901098901099], 'production': [0.024390243902439025, 0.02197802197802198], 'i': [0.07317073170731707, 0.02197802197802198], 'recommend': [0.024390243902439025, 0.01098901098901099], 'it': [0.07317073170731707, 0.03296703296703297], 'loved': [0.036585365853658534, 0.01098901098901099], 'my': [0.024390243902439025, 0.02197802197802198], 'children': [0.024390243902439025, 0.01098901098901099], 'really': [0.024390243902439025, 0.03296703296703297], 'enjoyed': [0.036585365853658534, 0.01098901098901099], 'amazing': [0.024390243902439025, 0.01098901098901099], 'had': [0.024390243902439025, 0.01098901098901099], 'a': [0.024390243902439025, 0.03296703296703297], 'good': [0.024390243902439025, 0.01098901098901099], 'time': [0.024390243902439025, 0.03296703296703297], 'so': [0.024390243902439025, 0.01098901098901099], 'much': [0.024390243902439025, 0.01098901098901099], 'beautiful': [0.024390243902439025, 0.01098901098901099], 'and': [0.024390243902439025, 0.06593406593406594], 'tender': [0.024390243902439025, 0.01098901098901099], 'story': [0.024390243902439025, 0.02197802197802198], 'boring': [0.012195121951219513, 0.054945054945054944], 'worst': [0.012195121951219513, 0.02197802197802198], 'waste': [0.012195121951219513, 0.03296703296703297], 'of': [0.012195121951219513, 0.03296703296703297], 'not': [0.012195121951219513, 0.03296703296703297], 'worth': [0.012195121951219513, 0.03296703296703297], 'pretentious': [0.012195121951219513, 0.02197802197802198], 'bad': [0.012195121951219513, 0.04395604395604396], 'want': [0.012195121951219513, 0.02197802197802198], 'money': [0.012195121951219513, 0.02197802197802198], 'back': [0.012195121951219513, 0.02197802197802198], 'terrible': [0.012195121951219513, 0.02197802197802198], 'mediocre': [0.012195121951219513, 0.03296703296703297], 'direction': [0.012195121951219513, 0.02197802197802198], 'very': [0.012195121951219513, 0.03296703296703297], 'tedious': [0.012195121951219513, 0.02197802197802198], 'characters': [0.012195121951219513, 0.02197802197802198]}\n"
     ]
    }
   ],
   "source": [
    "#word probability in positive and negative reviews\n",
    "\n",
    "word_frequency = calculate_word_frequency(words_in_positive_reviews, words_in_negative_reviews)\n",
    "print(word_frequency)\n",
    "\n",
    "word_probability = calculate_word_probability(word_frequency, words_in_positive_reviews, words_in_negative_reviews)\n",
    "print(word_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(text, positive_reviews, negative_reviews, probability):\n",
    "    #extract words from textt\n",
    "    words = extract_words(text)\n",
    "\n",
    "    #ratio of positive andnegative reviews\n",
    "    total_reviews = positive_reviews + negative_reviews\n",
    "\n",
    "    positive = positive_reviews / total_reviews\n",
    "    negative = negative_reviews / total_reviews\n",
    "\n",
    "    #probability of each word for positive and negative reviews\n",
    "    for word in words:\n",
    "        if word in probability.keys():\n",
    "            positive = positive * probability[word][0]\n",
    "            negative = negative * probability[word][1]\n",
    "    summation = positive + negative\n",
    "\n",
    "    return (positive/summation, negative/summation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in unseen reviews \n",
      "\n",
      "['beautiful', 'amazing', 'movie', 'i', 'loved', 'it']\n",
      "['boring', 'not', 'worth', 'it']\n",
      "['amazing', 'i', 'liked', 'it', 'so', 'much']\n",
      "['annoying', 'and', 'boring']\n",
      "['no', 'idea']\n",
      "\n",
      "Classification \n",
      "\n",
      "Beautiful, amazing movie, I loved it! is 0.9945 positive\n",
      "Boring, not worth it is 0.9368 negative\n",
      "Amazing, I liked it so much is 0.9878 positive\n",
      "Annoying and boring is 0.9241 negative\n",
      "No idea is neutral 0.5 positive 0.5 negative\n"
     ]
    }
   ],
   "source": [
    "   # test the classifier with unseen movie reviews\n",
    "\n",
    "unseen_reviews = [\"Beautiful, amazing movie, I loved it!\", \"Boring, not worth it\", \"Amazing, I liked it so much\", \"Annoying and boring\", \"No idea\"]\n",
    "\n",
    "print(\"Words in unseen reviews \\n\")\n",
    "\n",
    "for review in unseen_reviews:\n",
    "    print(extract_words(review))\n",
    "\n",
    "print(\"\\nClassification \\n\")\n",
    "\n",
    "for review in unseen_reviews:\n",
    "    result = classify(review, positive_reviews, negative_reviews, word_probability)\n",
    "\n",
    "    positive = result[0]\n",
    "    negative = result[1]\n",
    "\n",
    "    if abs(positive - negative) > 0.25:\n",
    "        if positive > negative:\n",
    "            print(review, \"is\", float(\"%.4f\" % positive), \"positive\")\n",
    "        else:\n",
    "            print(review, \"is\", float(\"%.4f\" % negative), \"negative\")\n",
    "    else:\n",
    "        print(review, \"is neutral\", float(\"%.4f\" % positive), \"positive\", float(\"%.4f\" % negative), \"negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"he'd\", 'an', 'theirs', 'being', 'down', 'll', 'mustn', \"should've\", 'when', 'their', 'or', \"we're\", 'these', 'y', 'am', 'needn', 'up', \"don't\", 'only', 'such', 'him', 'did', 'below', 'doing', \"it'd\", 'of', 'your', 'but', 'nor', 'over', 'under', 'any', \"i've\", \"wasn't\", 'them', 'from', 'while', 'so', 'who', 'shouldn', \"i'd\", 'does', 'have', \"i'm\", \"they'd\", 'as', \"it'll\", 'after', 'its', 'just', 'ours', 'this', 'ain', \"we'd\", 'if', 'same', \"they'll\", \"you've\", 'our', 'how', 'other', 'he', 'his', \"doesn't\", 'most', 'against', 'all', 'you', 'through', 'out', 'that', 't', 'wasn', \"we've\", 'having', 'yours', \"won't\", 'here', 'once', 'both', \"aren't\", 'at', 'there', 'whom', 'before', \"mightn't\", 'above', 's', \"needn't\", 'aren', 'why', 'to', 'had', 'more', 've', 'until', \"hadn't\", 'is', 'she', \"she'll\", 'which', 'those', 'be', 'few', 'between', 'during', \"mustn't\", 'too', 'ma', 'what', \"didn't\", 'each', \"hasn't\", 'd', \"wouldn't\", 'now', \"you'd\", \"she's\", 'some', 'themselves', 'shan', 'can', 'doesn', 'her', 'o', \"they're\", 'again', 'off', 'won', 'than', 'were', 'ourselves', 'and', \"you'll\", 'it', 're', 'didn', 'very', \"that'll\", 'should', 'where', 'herself', \"she'd\", 'they', 'weren', 'mightn', 'not', 'with', 'been', 'has', 'about', \"isn't\", 'on', 'will', 'hasn', 'i', 'a', 'itself', 'by', 'me', 'the', \"he's\", \"i'll\", \"shan't\", 'because', 'do', 'in', 'my', 'myself', 'hers', 'himself', 'for', \"they've\", 'we', 'are', 'no', 'hadn', \"couldn't\", 'wouldn', 'own', 'yourselves', \"shouldn't\", \"weren't\", 'yourself', 'further', 'm', \"he'll\", 'isn', 'was', \"you're\", \"haven't\", \"it's\", 'haven', 'then', \"we'll\", 'couldn', 'don', 'into'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "#set of english stop words\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the function extract words in ovveridden to exclude stop words\n",
    "\n",
    "def extract_words(text):\n",
    "    return list(\n",
    "        \n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
